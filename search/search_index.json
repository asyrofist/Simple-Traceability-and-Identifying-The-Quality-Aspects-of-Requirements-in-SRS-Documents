{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Simple-Traceability-SRS-Documents this is repository how to make simple traceability from requirement documents, you can check this out to live demo Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents described in our paper at EECCIS2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. @INPROCEEDINGS { 9263479 , author = { Asyrofi , Rakha and Hidayat , Taufik and Rochimah , Siti } , booktitle = { 2020 10 th Electrical Power , Electronics , Communications , Controls and Informatics Seminar ( EECCIS ) } , title = { Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents } , year = { 2020 } , pages = { 243 - 247 } , doi = { 10.1109 / EECCIS49483 .2020.9263479 }} Developed by Asyrofi (c) 2021 Cara menginstal instalasi melalui pypi: pip install tracereq Cara menggunakan program from tracereq.preprocessing_evaluation import prosesData myProses = prosesData ( inputData = 'dataset.xlsx' ) myProses . preprocessing () Check out this youtube link :","title":"Home"},{"location":"#simple-traceability-srs-documents","text":"this is repository how to make simple traceability from requirement documents, you can check this out to live demo Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents described in our paper at EECCIS2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. @INPROCEEDINGS { 9263479 , author = { Asyrofi , Rakha and Hidayat , Taufik and Rochimah , Siti } , booktitle = { 2020 10 th Electrical Power , Electronics , Communications , Controls and Informatics Seminar ( EECCIS ) } , title = { Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents } , year = { 2020 } , pages = { 243 - 247 } , doi = { 10.1109 / EECCIS49483 .2020.9263479 }} Developed by Asyrofi (c) 2021","title":"Simple-Traceability-SRS-Documents"},{"location":"#cara-menginstal","text":"instalasi melalui pypi: pip install tracereq","title":"Cara menginstal"},{"location":"#cara-menggunakan-program","text":"from tracereq.preprocessing_evaluation import prosesData myProses = prosesData ( inputData = 'dataset.xlsx' ) myProses . preprocessing () Check out this youtube link :","title":"Cara menggunakan program"},{"location":"CHANGELOG/","text":"Install the latest To install the latest version simply run: pip3 install tracereq see the Installation QuickStart for more instructions. Changelog 0.0.2 - Oct 9 2021 mengganti pemebersihan data menggunakan spacy module 0.0.1 - Sept 12 2021 menggunakan pembersihan data dengan nltk dapat melacak kebergantungan kebutuhan","title":"Changelog"},{"location":"CHANGELOG/#install-the-latest","text":"To install the latest version simply run: pip3 install tracereq see the Installation QuickStart for more instructions.","title":"Install the latest"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#002-oct-9-2021","text":"mengganti pemebersihan data menggunakan spacy module","title":"0.0.2 - Oct 9 2021"},{"location":"CHANGELOG/#001-sept-12-2021","text":"menggunakan pembersihan data dengan nltk dapat melacak kebergantungan kebutuhan","title":"0.0.1 - Sept 12 2021"},{"location":"TROUBLESHOOTING/","text":"Troubleshooting / FAQ Guide Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini No module not found Maka solusi yang bisa dibuat dengan cara yaitu melalui beberapa keterangan tahapan solusi dibawah ini. Solution 1: instalasi terlebih dahulu requirements.txt If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt Solution 2: Hubungi pihak admin apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat. Solution 3: Gunakanlah demostrasi Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda. Akhir Kata dari Penulis Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Troubleshooting"},{"location":"TROUBLESHOOTING/#troubleshooting-faq-guide","text":"Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini","title":"Troubleshooting / FAQ Guide"},{"location":"TROUBLESHOOTING/#no-module-not-found","text":"Maka solusi yang bisa dibuat dengan cara yaitu melalui beberapa keterangan tahapan solusi dibawah ini.","title":"No module not found"},{"location":"TROUBLESHOOTING/#solution-1-instalasi-terlebih-dahulu-requirementstxt","text":"If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt","title":"Solution 1: instalasi terlebih dahulu requirements.txt"},{"location":"TROUBLESHOOTING/#solution-2-hubungi-pihak-admin","text":"apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat.","title":"Solution 2: Hubungi pihak admin"},{"location":"TROUBLESHOOTING/#solution-3-gunakanlah-demostrasi","text":"Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda.","title":"Solution 3: Gunakanlah demostrasi"},{"location":"TROUBLESHOOTING/#akhir-kata-dari-penulis","text":"Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Akhir Kata dari Penulis"},{"location":"docs/kontribusi/","text":"Contributing to extractreq Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place. Getting portray set up for local development The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ requirements.txt bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/tracereq cd tracereq python setup.py Making a contribution Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :). Thank you! I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"1. Contributing Guide"},{"location":"docs/kontribusi/#contributing-to-extractreq","text":"Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place.","title":"Contributing to extractreq"},{"location":"docs/kontribusi/#getting-portray-set-up-for-local-development","text":"The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ requirements.txt bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/tracereq cd tracereq python setup.py","title":"Getting portray set up for local development"},{"location":"docs/kontribusi/#making-a-contribution","text":"Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :).","title":"Making a contribution"},{"location":"docs/kontribusi/#thank-you","text":"I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"Thank you!"},{"location":"docs/standard/","text":"HOPE 8 -- Style Guide for Hug Code HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019 Introduction This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference. PEP 8 Foundation All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines. Line Length Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters. Descriptive Variable names Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand. Adding new modules New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible. Automated Code Cleaners All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place. Automated Code Linting All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"2. Coding Standard"},{"location":"docs/standard/#hope-8-style-guide-for-hug-code","text":"HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019","title":"HOPE 8 -- Style Guide for Hug Code"},{"location":"docs/standard/#introduction","text":"This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference.","title":"Introduction"},{"location":"docs/standard/#pep-8-foundation","text":"All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines.","title":"PEP 8 Foundation"},{"location":"docs/standard/#line-length","text":"Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters.","title":"Line Length"},{"location":"docs/standard/#descriptive-variable-names","text":"Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand.","title":"Descriptive Variable names"},{"location":"docs/standard/#adding-new-modules","text":"New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible.","title":"Adding new modules"},{"location":"docs/standard/#automated-code-cleaners","text":"All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place.","title":"Automated Code Cleaners"},{"location":"docs/standard/#automated-code-linting","text":"All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"Automated Code Linting"},{"location":"reference/tracereq/","text":"Module tracereq None None View Source from tracereq.lda import latentDirichlet from tracereq.lsa import latentSemantic from tracereq.preprocessing_evaluation import ( pengukuranEvaluasi , prosesData ) from tracereq.vsm import measurement Sub-modules tracereq.lda tracereq.lsa tracereq.preprocessing_evaluation tracereq.vsm","title":"Index"},{"location":"reference/tracereq/#module-tracereq","text":"None None View Source from tracereq.lda import latentDirichlet from tracereq.lsa import latentSemantic from tracereq.preprocessing_evaluation import ( pengukuranEvaluasi , prosesData ) from tracereq.vsm import measurement","title":"Module tracereq"},{"location":"reference/tracereq/#sub-modules","text":"tracereq.lda tracereq.lsa tracereq.preprocessing_evaluation tracereq.vsm","title":"Sub-modules"},{"location":"reference/tracereq/lda/","text":"Module tracereq.lda None None View Source import pandas as pd from scipy.sparse import data from sklearn.decomposition import NMF , LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class latentDirichlet : def __init__ ( self , data_raw ): self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ): tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ): tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ): lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self )[ 1 ]) tf_feature_names = latentDirichlet . ukur_tf ( self )[ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def tabulasi_fr ( self , th , data ): dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) . ukur_evaluasi () return dt_fr , th_fr , myUkur def tabulasi_kl ( self , th , data ): dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) . ukur_evaluasi () return dt_kl , th_kl , myUkur def tabulasi_lda ( self , th , data ): dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) . ukur_evaluasi () return dt_lda , th_lda , myUkur def main ( self , th , data , output = [ 'fr' , 'kl' , 'lda' ]): if 'fr' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif 'kl' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif 'lda' in output : latentDirichlet . tabulasi_lda ( self , th , data ) if __name__ == \"__main__\" : try : a = latentDirichlet ( data ) . main ( 0.2 , data , 'lda' ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( a [ 2 ]) except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes latentDirichlet class latentDirichlet ( data_raw ) View Source class latentDirichlet : def __init__ ( self , data_raw ) : self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ) : tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def tabulasi_fr ( self , th , data ) : dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ). ukur_evaluasi () return dt_fr , th_fr , myUkur def tabulasi_kl ( self , th , data ) : dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ). ukur_evaluasi () return dt_kl , th_kl , myUkur def tabulasi_lda ( self , th , data ) : dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ). ukur_evaluasi () return dt_lda , th_lda , myUkur def main ( self , th , data , output = [ 'fr', 'kl', 'lda' ] ) : if 'fr' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif 'kl' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif 'lda' in output : latentDirichlet . tabulasi_lda ( self , th , data ) Methods Frobenius_norm_feature def Frobenius_norm_feature ( self , req ) View Source def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df Kullback_feature def Kullback_feature ( self , req ) View Source def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df lda_feature def lda_feature ( self , req ) View Source def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ) , max_iter = 5 , learning_method = ' online ' , learning_offset = 50 ., random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ]. get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df main def main ( self , th , data , output = [ 'fr' , 'kl' , 'lda' ] ) View Source def main ( self , th , data , output = [ ' fr ' , ' kl ' , ' lda ' ] ) : if ' fr ' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif ' kl ' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif ' lda ' in output : latentDirichlet . tabulasi_lda ( self , th , data ) tabulasi_fr def tabulasi_fr ( self , th , data ) View Source def tabulasi_fr ( self , th , data ) : dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) . ukur_evaluasi () return dt_fr , th_fr , myUkur tabulasi_kl def tabulasi_kl ( self , th , data ) View Source def tabulasi_kl ( self , th , data ) : dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) . ukur_evaluasi () return dt_kl , th_kl , myUkur tabulasi_lda def tabulasi_lda ( self , th , data ) View Source def tabulasi_lda ( self , th , data ) : dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) . ukur_evaluasi () return dt_lda , th_lda , myUkur threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 ukur_tf def ukur_tf ( self ) View Source def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf ukur_tfidf_vectorizer def ukur_tfidf_vectorizer ( self ) View Source def ukur_tfidf_vectorizer ( self ) : tfidf_vectorizer = TfidfVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf","title":"Lda"},{"location":"reference/tracereq/lda/#module-tracereqlda","text":"None None View Source import pandas as pd from scipy.sparse import data from sklearn.decomposition import NMF , LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class latentDirichlet : def __init__ ( self , data_raw ): self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ): tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ): tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ): lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self )[ 1 ]) tf_feature_names = latentDirichlet . ukur_tf ( self )[ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def tabulasi_fr ( self , th , data ): dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) . ukur_evaluasi () return dt_fr , th_fr , myUkur def tabulasi_kl ( self , th , data ): dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) . ukur_evaluasi () return dt_kl , th_kl , myUkur def tabulasi_lda ( self , th , data ): dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) . ukur_evaluasi () return dt_lda , th_lda , myUkur def main ( self , th , data , output = [ 'fr' , 'kl' , 'lda' ]): if 'fr' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif 'kl' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif 'lda' in output : latentDirichlet . tabulasi_lda ( self , th , data ) if __name__ == \"__main__\" : try : a = latentDirichlet ( data ) . main ( 0.2 , data , 'lda' ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( a [ 2 ]) except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module tracereq.lda"},{"location":"reference/tracereq/lda/#classes","text":"","title":"Classes"},{"location":"reference/tracereq/lda/#latentdirichlet","text":"class latentDirichlet ( data_raw ) View Source class latentDirichlet : def __init__ ( self , data_raw ) : self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ) : tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def tabulasi_fr ( self , th , data ) : dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ). ukur_evaluasi () return dt_fr , th_fr , myUkur def tabulasi_kl ( self , th , data ) : dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ). ukur_evaluasi () return dt_kl , th_kl , myUkur def tabulasi_lda ( self , th , data ) : dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ). ukur_evaluasi () return dt_lda , th_lda , myUkur def main ( self , th , data , output = [ 'fr', 'kl', 'lda' ] ) : if 'fr' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif 'kl' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif 'lda' in output : latentDirichlet . tabulasi_lda ( self , th , data )","title":"latentDirichlet"},{"location":"reference/tracereq/lda/#methods","text":"","title":"Methods"},{"location":"reference/tracereq/lda/#frobenius_norm_feature","text":"def Frobenius_norm_feature ( self , req ) View Source def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df","title":"Frobenius_norm_feature"},{"location":"reference/tracereq/lda/#kullback_feature","text":"def Kullback_feature ( self , req ) View Source def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df","title":"Kullback_feature"},{"location":"reference/tracereq/lda/#lda_feature","text":"def lda_feature ( self , req ) View Source def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ) , max_iter = 5 , learning_method = ' online ' , learning_offset = 50 ., random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ]. get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df","title":"lda_feature"},{"location":"reference/tracereq/lda/#main","text":"def main ( self , th , data , output = [ 'fr' , 'kl' , 'lda' ] ) View Source def main ( self , th , data , output = [ ' fr ' , ' kl ' , ' lda ' ] ) : if ' fr ' in output : latentDirichlet . tabulasi_fr ( self , th , data ) elif ' kl ' in output : latentDirichlet . tabulasi_kl ( self , th , data ) elif ' lda ' in output : latentDirichlet . tabulasi_lda ( self , th , data )","title":"main"},{"location":"reference/tracereq/lda/#tabulasi_fr","text":"def tabulasi_fr ( self , th , data ) View Source def tabulasi_fr ( self , th , data ) : dt_fr = latentDirichlet . Frobenius_norm_feature ( self , data ) th_fr = latentDirichlet . threshold_value ( self , th , dt_fr ) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) . ukur_evaluasi () return dt_fr , th_fr , myUkur","title":"tabulasi_fr"},{"location":"reference/tracereq/lda/#tabulasi_kl","text":"def tabulasi_kl ( self , th , data ) View Source def tabulasi_kl ( self , th , data ) : dt_kl = latentDirichlet . Kullback_feature ( self , data ) th_kl = latentDirichlet . threshold_value ( self , th , dt_kl ) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) . ukur_evaluasi () return dt_kl , th_kl , myUkur","title":"tabulasi_kl"},{"location":"reference/tracereq/lda/#tabulasi_lda","text":"def tabulasi_lda ( self , th , data ) View Source def tabulasi_lda ( self , th , data ) : dt_lda = latentDirichlet . lda_feature ( data ) th_lda = latentDirichlet . threshold_value ( th , dt_lda ) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) . ukur_evaluasi () return dt_lda , th_lda , myUkur","title":"tabulasi_lda"},{"location":"reference/tracereq/lda/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"},{"location":"reference/tracereq/lda/#ukur_tf","text":"def ukur_tf ( self ) View Source def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf","title":"ukur_tf"},{"location":"reference/tracereq/lda/#ukur_tfidf_vectorizer","text":"def ukur_tfidf_vectorizer ( self ) View Source def ukur_tfidf_vectorizer ( self ) : tfidf_vectorizer = TfidfVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf","title":"ukur_tfidf_vectorizer"},{"location":"reference/tracereq/lsa/","text":"Module tracereq.lsa None None View Source import pandas as pd from scipy.sparse import data from sklearn.decomposition import TruncatedSVD from sklearn.feature_extraction.text import TfidfVectorizer from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class latentSemantic : def __init__ ( self , data_raw ): self . data = data_raw def ukurLSA ( self , req ): vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data ): dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) . ukur_evaluasi () return dt_lsa , th_lsa , myUkur if __name__ == \"__main__\" : try : a = latentSemantic ( data ) . main ( 0.2 , data ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( a [ 2 ]) except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes latentSemantic class latentSemantic ( data_raw ) View Source class latentSemantic : def __init__ ( self , data_raw ) : self . data = data_raw def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data ) : dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ). ukur_evaluasi () return dt_lsa , th_lsa , myUkur Methods main def main ( self , threshold , data ) View Source def main ( self , threshold , data ) : dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) . ukur_evaluasi () return dt_lsa , th_lsa , myUkur threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 ukurLSA def ukurLSA ( self , req ) View Source def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = ' english ' , max_features = 1000 , # keep top 1000 terms max_df = 0 . 5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ) , algorithm = ' randomized ' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms )","title":"Lsa"},{"location":"reference/tracereq/lsa/#module-tracereqlsa","text":"None None View Source import pandas as pd from scipy.sparse import data from sklearn.decomposition import TruncatedSVD from sklearn.feature_extraction.text import TfidfVectorizer from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class latentSemantic : def __init__ ( self , data_raw ): self . data = data_raw def ukurLSA ( self , req ): vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data ): dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) . ukur_evaluasi () return dt_lsa , th_lsa , myUkur if __name__ == \"__main__\" : try : a = latentSemantic ( data ) . main ( 0.2 , data ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( a [ 2 ]) except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module tracereq.lsa"},{"location":"reference/tracereq/lsa/#classes","text":"","title":"Classes"},{"location":"reference/tracereq/lsa/#latentsemantic","text":"class latentSemantic ( data_raw ) View Source class latentSemantic : def __init__ ( self , data_raw ) : self . data = data_raw def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data ) : dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ). ukur_evaluasi () return dt_lsa , th_lsa , myUkur","title":"latentSemantic"},{"location":"reference/tracereq/lsa/#methods","text":"","title":"Methods"},{"location":"reference/tracereq/lsa/#main","text":"def main ( self , threshold , data ) View Source def main ( self , threshold , data ) : dt_lsa = latentSemantic . ukurLSA ( self , data ) th_lsa = latentSemantic . threshold_value ( self , threshold , dt_lsa ) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) . ukur_evaluasi () return dt_lsa , th_lsa , myUkur","title":"main"},{"location":"reference/tracereq/lsa/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"},{"location":"reference/tracereq/lsa/#ukurlsa","text":"def ukurLSA ( self , req ) View Source def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = ' english ' , max_features = 1000 , # keep top 1000 terms max_df = 0 . 5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ) , algorithm = ' randomized ' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms )","title":"ukurLSA"},{"location":"reference/tracereq/preprocessing_evaluation/","text":"Module tracereq.preprocessing_evaluation None None View Source import numpy as np , pandas as pd from pyAutoML.ml import ML , EncodeCategorical , ml from sklearn.cluster import KMeans from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import MinMaxScaler from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from spacy.lang.en import English from tabulate import tabulate class prosesData : def __init__ ( self , namaFile ): '''fungi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ): '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): '''fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ): '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) ''' cleaned_X = [ prosesData . clean_text ( self , raw_text = element ) for element in X ] return cleaned_X def clean_text ( self , raw_text ): '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ): '''fungsi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ): '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2: Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )): predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all (): correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {} \" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ): X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70% training and 30% test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0.4 return ML ( X_train , y_train , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) if __name__ == \"__main__\" : myData = prosesData () # myData.preprocessing() req = myData . fulldataset () # myData.fulldataset(inputSRS) text_to_clean = list ( req [ 'Requirement Statement' ]) cleaned_text = myData . apply_cleaning_function_to_list ( text_to_clean ) Classes pengukuranEvaluasi class pengukuranEvaluasi ( dataPertama , dataKedua ) View Source class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ) : '''fungsi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ) : '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0.4 return ML ( X_train , y_train , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) Methods kmeans_cluster def kmeans_cluster ( self , nilai_cluster = 3 ) fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() View Source def kmeans_cluster ( self , nilai_cluster = 3 ) : '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) ukur_evaluasi def ukur_evaluasi ( self ) View Source def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ) , self . data2 , test_size = 0 . 3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0 . 4 return ML ( X_train , y_train , size , SVC () , RandomForestClassifier () , DecisionTreeClassifier () , KNeighborsClassifier () , LogisticRegression ( max_iter = 7000 )) prosesData class prosesData ( namaFile ) View Source class prosesData : def __init__ ( self , namaFile ) : '''fungi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ) : '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : '''fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ) : '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) ''' cleaned_X = [ prosesData.clean_text(self, raw_text= element)for element in X ] return cleaned_X def clean_text ( self , raw_text ) : '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token.lemma_.lower() for token in tokens if token.is_stop is False and token.is_punct is False and token.is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words Methods apply_cleaning_function_to_list def apply_cleaning_function_to_list ( self , X ) fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) View Source def apply_cleaning_function_to_list ( self , X ) : ''' fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih . Sehingga maasuk ke tahapan selanjutnya . prosesData ( data ) . apply_cleaning_function_to_list ( X ) ''' cleaned_X = [ prosesData . clean_text ( self , raw_text = element ) for element in X ] return cleaned_X clean_text def clean_text ( self , raw_text ) fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) View Source def clean_text ( self , raw_text ) : ''' fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih . Sehingga maasuk ke tahapan selanjutnya . prosesData ( data ) . clean_text ( raw_text ) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words fulldataset def fulldataset ( self , inputSRS = 'SRS1' ) fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS = 'SRS1' ) : '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs preprocessing def preprocessing ( self ) fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() View Source def preprocessing ( self ) : ''' fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan . sehingga syntax yang digunakan yaitu adalah sebagai berikut : prosesData ( data ) . preprocessing () ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"Preprocessing Evaluation"},{"location":"reference/tracereq/preprocessing_evaluation/#module-tracereqpreprocessing_evaluation","text":"None None View Source import numpy as np , pandas as pd from pyAutoML.ml import ML , EncodeCategorical , ml from sklearn.cluster import KMeans from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import MinMaxScaler from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from spacy.lang.en import English from tabulate import tabulate class prosesData : def __init__ ( self , namaFile ): '''fungi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ): '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names }[ inputSRS ] return dfs def preprocessing ( self ): '''fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ): '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) ''' cleaned_X = [ prosesData . clean_text ( self , raw_text = element ) for element in X ] return cleaned_X def clean_text ( self , raw_text ): '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ): '''fungsi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ): '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2: Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )): predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all (): correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {} \" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ): X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70% training and 30% test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0.4 return ML ( X_train , y_train , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) if __name__ == \"__main__\" : myData = prosesData () # myData.preprocessing() req = myData . fulldataset () # myData.fulldataset(inputSRS) text_to_clean = list ( req [ 'Requirement Statement' ]) cleaned_text = myData . apply_cleaning_function_to_list ( text_to_clean )","title":"Module tracereq.preprocessing_evaluation"},{"location":"reference/tracereq/preprocessing_evaluation/#classes","text":"","title":"Classes"},{"location":"reference/tracereq/preprocessing_evaluation/#pengukuranevaluasi","text":"class pengukuranEvaluasi ( dataPertama , dataKedua ) View Source class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ) : '''fungsi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ) : '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0.4 return ML ( X_train , y_train , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 ))","title":"pengukuranEvaluasi"},{"location":"reference/tracereq/preprocessing_evaluation/#methods","text":"","title":"Methods"},{"location":"reference/tracereq/preprocessing_evaluation/#kmeans_cluster","text":"def kmeans_cluster ( self , nilai_cluster = 3 ) fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() View Source def kmeans_cluster ( self , nilai_cluster = 3 ) : '''fungsi ini digunakan untuk mengklaster data menggunakan metode kmeans. Dari fungsi ini dapat menciptakan data korelasi yang dimuat hasil klaster. pengukuranEvaluasi() ''' XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled )","title":"kmeans_cluster"},{"location":"reference/tracereq/preprocessing_evaluation/#ukur_evaluasi","text":"def ukur_evaluasi ( self ) View Source def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = \\ train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ) , self . data2 , test_size = 0 . 3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) y_train = EncodeCategorical ( y_train ) size = 0 . 4 return ML ( X_train , y_train , size , SVC () , RandomForestClassifier () , DecisionTreeClassifier () , KNeighborsClassifier () , LogisticRegression ( max_iter = 7000 ))","title":"ukur_evaluasi"},{"location":"reference/tracereq/preprocessing_evaluation/#prosesdata","text":"class prosesData ( namaFile ) View Source class prosesData : def __init__ ( self , namaFile ) : '''fungi ini digunakan untuk menginisialisasi data sehingga perlu dilakukan proses construct data. ''' self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ) : '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs def preprocessing ( self ) : '''fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ) : '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) ''' cleaned_X = [ prosesData.clean_text(self, raw_text= element)for element in X ] return cleaned_X def clean_text ( self , raw_text ) : '''fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token.lemma_.lower() for token in tokens if token.is_stop is False and token.is_punct is False and token.is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words","title":"prosesData"},{"location":"reference/tracereq/preprocessing_evaluation/#methods_1","text":"","title":"Methods"},{"location":"reference/tracereq/preprocessing_evaluation/#apply_cleaning_function_to_list","text":"def apply_cleaning_function_to_list ( self , X ) fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).apply_cleaning_function_to_list(X) View Source def apply_cleaning_function_to_list ( self , X ) : ''' fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih . Sehingga maasuk ke tahapan selanjutnya . prosesData ( data ) . apply_cleaning_function_to_list ( X ) ''' cleaned_X = [ prosesData . clean_text ( self , raw_text = element ) for element in X ] return cleaned_X","title":"apply_cleaning_function_to_list"},{"location":"reference/tracereq/preprocessing_evaluation/#clean_text","text":"def clean_text ( self , raw_text ) fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih. Sehingga maasuk ke tahapan selanjutnya. prosesData(data).clean_text(raw_text) View Source def clean_text ( self , raw_text ) : ''' fungsi ini digunakan untuk mengaplikasikan fungsi pembersihan data, sesuai dengan list yang dipilih . Sehingga maasuk ke tahapan selanjutnya . prosesData ( data ) . clean_text ( raw_text ) ''' nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words","title":"clean_text"},{"location":"reference/tracereq/preprocessing_evaluation/#fulldataset","text":"def fulldataset ( self , inputSRS = 'SRS1' ) fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) View Source def fulldataset ( self , inputSRS = 'SRS1' ) : '''fungi ini digunakan untuk menampilkan data secara spesifik, sehingga menunjukkan data yang diinginkan. syntax yang digunakan adalah sebagai berikut: prosesData(data).fulldataset(inputSRS) ''' xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } [ inputSRS ] return dfs","title":"fulldataset"},{"location":"reference/tracereq/preprocessing_evaluation/#preprocessing","text":"def preprocessing ( self ) fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan. sehingga syntax yang digunakan yaitu adalah sebagai berikut: prosesData(data).preprocessing() View Source def preprocessing ( self ) : ''' fungsi ini digunakan untuk mengecek data yang digunakan untuk melihat data yang digunakan . sehingga syntax yang digunakan yaitu adalah sebagai berikut : prosesData ( data ) . preprocessing () ''' xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/tracereq/vsm/","text":"Module tracereq.vsm None None View Source import math , numpy as np , pandas as pd from scipy.sparse import data from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels from sklearn.preprocessing import MinMaxScaler from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class measurement : def __init__ ( self , cleaned_data ): self . data = cleaned_data def bagOfWords ( self , data_raw , req ): b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya, sebagai kolom e = b . transform ( data_raw ) . toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data, indeks, kolom return bow_df def l2_normalizer ( self , vec ): denom = np . sum ([ el ** 2 for el in vec ]) return [( el / math . sqrt ( denom )) for el in vec ] def build_lexicon ( self , corpus ): lexicon = set () for doc in corpus : lexicon . update ([ word for word in doc . split ()]) return lexicon def freq ( self , term , document ): return document . split () . count ( term ) def numDocsContaining ( self , word , doclist ): doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ): n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ): idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ): X = np . array ( data [ 0 :]) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data , output = [ 'normal' , 'l2_normalizer' ]): bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement . idf ( self , word , data ) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) . ukur_evaluasi () doc_term_matrix_l2 = [ measurement . l2_normalizer ( self , vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ measurement . l2_normalizer ( self , tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) . ukur_evaluasi () if 'normal' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if 'l2_normalizer' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur if __name__ == \"__main__\" : try : a = measurement ( data ) . main ( 0.2 , data , 'l2_normalizer' ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 2 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 3 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 4 ], headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0} \" . format ( err )) Classes measurement class measurement ( cleaned_data ) View Source class measurement : def __init__ ( self , cleaned_data ) : self . data = cleaned_data def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ). toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data , indeks , kolom return bow_df def l2_normalizer ( self , vec ) : denom = np . sum ( [ el**2 for el in vec ] ) return [ (el / math.sqrt(denom)) for el in vec ] def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc.split() ] ) return lexicon def freq ( self , term , document ) : return document . split (). count ( term ) def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0: ] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data , output = [ 'normal', 'l2_normalizer' ] ) : bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement.idf(self, word, data) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np.dot(tf_vector, my_idf_matrix) for tf_vector in bow.values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ). ukur_evaluasi () doc_term_matrix_l2 = [ measurement.l2_normalizer(self, vec) for vec in bow.values ] doc_term_matrix_tfidf_l2 = [ measurement.l2_normalizer(self, tf_vector) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ). ukur_evaluasi () if 'normal' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if 'l2_normalizer' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur Methods bagOfWords def bagOfWords ( self , data_raw , req ) View Source def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ) . toarray () # data bow_df = pd . DataFrame ( e , req , d ) # data , indeks , kolom return bow_df build_idf_matrix def build_idf_matrix ( self , idf_vector ) View Source def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ) , len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat build_lexicon def build_lexicon ( self , corpus ) View Source def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc . split () ] ) return lexicon cosine_measurement def cosine_measurement ( self , data , req ) View Source def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0 :] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = ' linear ' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine freq def freq ( self , term , document ) View Source def freq ( self , term , document ) : return document . split () . count ( term ) idf def idf ( self , word , doclist ) View Source def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) l2_normalizer def l2_normalizer ( self , vec ) View Source def l2_normalizer ( self , vec ) : denom = np . sum ( [ el ** 2 for el in vec ] ) return [ ( el / math . sqrt ( denom )) for el in vec ] main def main ( self , threshold , data , output = [ 'normal' , 'l2_normalizer' ] ) View Source def main ( self , threshold , data , output = [ ' normal ' , ' l2_normalizer ' ] ) : bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement . idf ( self , word , data ) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) . ukur_evaluasi () doc_term_matrix_l2 = [ measurement . l2_normalizer ( self , vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ measurement . l2_normalizer ( self , tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) . ukur_evaluasi () if ' normal ' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if ' l2_normalizer ' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur numDocsContaining def numDocsContaining ( self , word , doclist ) View Source def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"Vsm"},{"location":"reference/tracereq/vsm/#module-tracereqvsm","text":"None None View Source import math , numpy as np , pandas as pd from scipy.sparse import data from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels from sklearn.preprocessing import MinMaxScaler from tabulate import tabulate from tracereq.preprocessing_evaluation import pengukuranEvaluasi class measurement : def __init__ ( self , cleaned_data ): self . data = cleaned_data def bagOfWords ( self , data_raw , req ): b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya, sebagai kolom e = b . transform ( data_raw ) . toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data, indeks, kolom return bow_df def l2_normalizer ( self , vec ): denom = np . sum ([ el ** 2 for el in vec ]) return [( el / math . sqrt ( denom )) for el in vec ] def build_lexicon ( self , corpus ): lexicon = set () for doc in corpus : lexicon . update ([ word for word in doc . split ()]) return lexicon def freq ( self , term , document ): return document . split () . count ( term ) def numDocsContaining ( self , word , doclist ): doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ): n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ): idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ): X = np . array ( data [ 0 :]) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data , output = [ 'normal' , 'l2_normalizer' ]): bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement . idf ( self , word , data ) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) . ukur_evaluasi () doc_term_matrix_l2 = [ measurement . l2_normalizer ( self , vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ measurement . l2_normalizer ( self , tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) . ukur_evaluasi () if 'normal' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if 'l2_normalizer' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur if __name__ == \"__main__\" : try : a = measurement ( data ) . main ( 0.2 , data , 'l2_normalizer' ) print ( tabulate ( a [ 0 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 1 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 2 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 3 ], headers = 'keys' , tablefmt = 'psql' )) print ( tabulate ( a [ 4 ], headers = 'keys' , tablefmt = 'psql' )) except OSError as err : print ( \"OS error: {0} \" . format ( err ))","title":"Module tracereq.vsm"},{"location":"reference/tracereq/vsm/#classes","text":"","title":"Classes"},{"location":"reference/tracereq/vsm/#measurement","text":"class measurement ( cleaned_data ) View Source class measurement : def __init__ ( self , cleaned_data ) : self . data = cleaned_data def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ). toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data , indeks , kolom return bow_df def l2_normalizer ( self , vec ) : denom = np . sum ( [ el**2 for el in vec ] ) return [ (el / math.sqrt(denom)) for el in vec ] def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc.split() ] ) return lexicon def freq ( self , term , document ) : return document . split (). count ( term ) def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0: ] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 def main ( self , threshold , data , output = [ 'normal', 'l2_normalizer' ] ) : bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement.idf(self, word, data) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np.dot(tf_vector, my_idf_matrix) for tf_vector in bow.values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ). ukur_evaluasi () doc_term_matrix_l2 = [ measurement.l2_normalizer(self, vec) for vec in bow.values ] doc_term_matrix_tfidf_l2 = [ measurement.l2_normalizer(self, tf_vector) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ). ukur_evaluasi () if 'normal' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if 'l2_normalizer' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur","title":"measurement"},{"location":"reference/tracereq/vsm/#methods","text":"","title":"Methods"},{"location":"reference/tracereq/vsm/#bagofwords","text":"def bagOfWords ( self , data_raw , req ) View Source def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ) . toarray () # data bow_df = pd . DataFrame ( e , req , d ) # data , indeks , kolom return bow_df","title":"bagOfWords"},{"location":"reference/tracereq/vsm/#build_idf_matrix","text":"def build_idf_matrix ( self , idf_vector ) View Source def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ) , len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat","title":"build_idf_matrix"},{"location":"reference/tracereq/vsm/#build_lexicon","text":"def build_lexicon ( self , corpus ) View Source def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc . split () ] ) return lexicon","title":"build_lexicon"},{"location":"reference/tracereq/vsm/#cosine_measurement","text":"def cosine_measurement ( self , data , req ) View Source def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0 :] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = ' linear ' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine","title":"cosine_measurement"},{"location":"reference/tracereq/vsm/#freq","text":"def freq ( self , term , document ) View Source def freq ( self , term , document ) : return document . split () . count ( term )","title":"freq"},{"location":"reference/tracereq/vsm/#idf","text":"def idf ( self , word , doclist ) View Source def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df )","title":"idf"},{"location":"reference/tracereq/vsm/#l2_normalizer","text":"def l2_normalizer ( self , vec ) View Source def l2_normalizer ( self , vec ) : denom = np . sum ( [ el ** 2 for el in vec ] ) return [ ( el / math . sqrt ( denom )) for el in vec ]","title":"l2_normalizer"},{"location":"reference/tracereq/vsm/#main","text":"def main ( self , threshold , data , output = [ 'normal' , 'l2_normalizer' ] ) View Source def main ( self , threshold , data , output = [ ' normal ' , ' l2_normalizer ' ] ) : bow = measurement . bagOfWords ( self , data ) vocabulary = measurement . build_lexicon ( self , data ) my_idf_vector = [ measurement . idf ( self , word , data ) for word in vocabulary ] # vektor idf my_idf_matrix = measurement . build_idf_matrix ( self , my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = measurement . cosine_measurement ( self , doc_term_matrix_tfidf . values ) th_cosine = measurement . threshold_value ( threshold , dt_cosine ) # tfidf normal myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) . ukur_evaluasi () doc_term_matrix_l2 = [ measurement . l2_normalizer ( self , vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ measurement . l2_normalizer ( self , tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = measurement . cosine_measurement ( self , doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = measurement . threshold_value ( self , threshold , dt_cosine_l2 ) # tfidf dengan l2 normalizer myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) . ukur_evaluasi () if ' normal ' in output : return my_idf_vector , my_idf_matrix , doc_term_matrix_tfidf , dt_cosine , th_cosine , myUkur if ' l2_normalizer ' in output : return doc_term_matrix_l2 , doc_term_matrix_tfidf_l2 , dt_cosine_l2 , th_cosine_l2 , myUkur","title":"main"},{"location":"reference/tracereq/vsm/#numdocscontaining","text":"def numDocsContaining ( self , word , doclist ) View Source def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount","title":"numDocsContaining"},{"location":"reference/tracereq/vsm/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"}]}