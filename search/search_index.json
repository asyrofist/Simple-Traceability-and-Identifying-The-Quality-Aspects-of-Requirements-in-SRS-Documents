{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Simple-Traceability-SRS-Documents this is repository how to make simple traceability from requirement documents, you can check this out to live demo Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents described in our paper at EECCIS2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. @INPROCEEDINGS { 9263479 , author = { Asyrofi , Rakha and Hidayat , Taufik and Rochimah , Siti } , booktitle = { 2020 10 th Electrical Power , Electronics , Communications , Controls and Informatics Seminar ( EECCIS ) } , title = { Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents } , year = { 2020 } , pages = { 243 - 247 } , doi = { 10.1109 / EECCIS49483 .2020.9263479 }} Developed by Asyrofi (c) 2021 Cara menginstal instalasi melalui pypi: pip install tracereq Cara menggunakan program from tracereq.preprocessing_evaluation import prosesData myProses = prosesData ( inputData = 'dataset.xlsx' ) myProses . preprocessing () Check out this youtube link :","title":"Home"},{"location":"#simple-traceability-srs-documents","text":"this is repository how to make simple traceability from requirement documents, you can check this out to live demo Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents described in our paper at EECCIS2020 . Please kindly cite the following paper when you use this tool. It would also be appreciated if you send me a courtesy website and google scholar , so I could survey what kind of tasks the tool is used for. @INPROCEEDINGS { 9263479 , author = { Asyrofi , Rakha and Hidayat , Taufik and Rochimah , Siti } , booktitle = { 2020 10 th Electrical Power , Electronics , Communications , Controls and Informatics Seminar ( EECCIS ) } , title = { Comparative Studies of Several Methods for Building Simple Traceability and Identifying The Quality Aspects of Requirements in SRS Documents } , year = { 2020 } , pages = { 243 - 247 } , doi = { 10.1109 / EECCIS49483 .2020.9263479 }} Developed by Asyrofi (c) 2021","title":"Simple-Traceability-SRS-Documents"},{"location":"#cara-menginstal","text":"instalasi melalui pypi: pip install tracereq","title":"Cara menginstal"},{"location":"#cara-menggunakan-program","text":"from tracereq.preprocessing_evaluation import prosesData myProses = prosesData ( inputData = 'dataset.xlsx' ) myProses . preprocessing () Check out this youtube link :","title":"Cara menggunakan program"},{"location":"CHANGELOG/","text":"Install the latest To install the latest version simply run: pip3 install tracereq see the Installation QuickStart for more instructions. Changelog 0.0.2 - Oct 9 2021 mengganti pemebersihan data menggunakan spacy module 0.0.1 - Sept 12 2021 menggunakan pembersihan data dengan nltk dapat melacak kebergantungan kebutuhan","title":"Changelog"},{"location":"CHANGELOG/#install-the-latest","text":"To install the latest version simply run: pip3 install tracereq see the Installation QuickStart for more instructions.","title":"Install the latest"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#002-oct-9-2021","text":"mengganti pemebersihan data menggunakan spacy module","title":"0.0.2 - Oct 9 2021"},{"location":"CHANGELOG/#001-sept-12-2021","text":"menggunakan pembersihan data dengan nltk dapat melacak kebergantungan kebutuhan","title":"0.0.1 - Sept 12 2021"},{"location":"TROUBLESHOOTING/","text":"Troubleshooting / FAQ Guide Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini No module not found Maka solusi yang bisa dibuat dengan cara yaitu melalui beberapa keterangan tahapan solusi dibawah ini. Solution 1: instalasi terlebih dahulu requirements.txt If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt Solution 2: Hubungi pihak admin apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat. Solution 3: Gunakanlah demostrasi Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda. Akhir Kata dari Penulis Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Troubleshooting"},{"location":"TROUBLESHOOTING/#troubleshooting-faq-guide","text":"Bagaimana jika menemukan masalah, seperti ini dikemudian hari, apabila saat menggunakan package ini. Maka muncul error seperti ini","title":"Troubleshooting / FAQ Guide"},{"location":"TROUBLESHOOTING/#no-module-not-found","text":"Maka solusi yang bisa dibuat dengan cara yaitu melalui beberapa keterangan tahapan solusi dibawah ini.","title":"No module not found"},{"location":"TROUBLESHOOTING/#solution-1-instalasi-terlebih-dahulu-requirementstxt","text":"If you do have a requirements.txt sehingga cukup instalasi dengan cara menginstal semua modul yang dibutuhkan dalam package ini. Dengan cara pip install - r requirements . txt","title":"Solution 1: instalasi terlebih dahulu requirements.txt"},{"location":"TROUBLESHOOTING/#solution-2-hubungi-pihak-admin","text":"apabila dikemudian hari, anda menemukan masalah yang lebih spesifik. Alangkah lebih baiknya, anda cukup menghubungi asyrofi.19051@mhs.its.ac.id agar segera ditangani dan dikerjakan secara cepat.","title":"Solution 2: Hubungi pihak admin"},{"location":"TROUBLESHOOTING/#solution-3-gunakanlah-demostrasi","text":"Apabila, anda hanya mencoba aplikasi yang digunakan. Ada baiknya, sekarang anda tinggal mengklik tombol live demo dari keterangan deskripsi README.md . Sehingga agar dapat mempermudah anda.","title":"Solution 3: Gunakanlah demostrasi"},{"location":"TROUBLESHOOTING/#akhir-kata-dari-penulis","text":"Baik itu saja, yang akan saya sampaikan. Semoga tidak ada kesalahan, atau error yang berarti dalam menggunakan package ini. Baik, saya ucapkan happy coding.. semoga bermanfaat","title":"Akhir Kata dari Penulis"},{"location":"docs/kontribusi/","text":"Contributing to extractreq Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place. Getting portray set up for local development The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ requirements.txt bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/tracereq cd tracereq python setup.py Making a contribution Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :). Thank you! I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"1. Contributing Guide"},{"location":"docs/kontribusi/#contributing-to-extractreq","text":"Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place.","title":"Contributing to extractreq"},{"location":"docs/kontribusi/#getting-portray-set-up-for-local-development","text":"The first step when contributing to any project is getting it set up on your local machine. portray aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ requirements.txt bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/asyrofist/tracereq cd tracereq python setup.py","title":"Getting portray set up for local development"},{"location":"docs/kontribusi/#making-a-contribution","text":"Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main portray project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :).","title":"Making a contribution"},{"location":"docs/kontribusi/#thank-you","text":"I can not tell you how thankful I am for the hard work done by portray contributors like you . Thank you! ~Rakha Asyrofi","title":"Thank you!"},{"location":"docs/standard/","text":"HOPE 8 -- Style Guide for Hug Code HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019 Introduction This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference. PEP 8 Foundation All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines. Line Length Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters. Descriptive Variable names Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand. Adding new modules New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible. Automated Code Cleaners All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place. Automated Code Linting All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"2. Coding Standard"},{"location":"docs/standard/#hope-8-style-guide-for-hug-code","text":"HOPE: 8 Title: Style Guide for Hug Code Author(s): Rakha Asyrofi asyrofi.19051@mhs.its.ac.id Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019","title":"HOPE 8 -- Style Guide for Hug Code"},{"location":"docs/standard/#introduction","text":"This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference.","title":"Introduction"},{"location":"docs/standard/#pep-8-foundation","text":"All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines.","title":"PEP 8 Foundation"},{"location":"docs/standard/#line-length","text":"Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters.","title":"Line Length"},{"location":"docs/standard/#descriptive-variable-names","text":"Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand.","title":"Descriptive Variable names"},{"location":"docs/standard/#adding-new-modules","text":"New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible.","title":"Adding new modules"},{"location":"docs/standard/#automated-code-cleaners","text":"All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place.","title":"Automated Code Cleaners"},{"location":"docs/standard/#automated-code-linting","text":"All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"Automated Code Linting"},{"location":"reference/traceability/","text":"Module traceability None None View Source from traceability.preprocessing_evaluation import prosesData , pengukuranEvaluasi from traceability.lsa import latentSemantic from traceability.lda import latentDirichlet from traceability.vsm import measurement Sub-modules traceability.lda traceability.lsa traceability.preprocessing_evaluation traceability.vsm","title":"Index"},{"location":"reference/traceability/#module-traceability","text":"None None View Source from traceability.preprocessing_evaluation import prosesData , pengukuranEvaluasi from traceability.lsa import latentSemantic from traceability.lda import latentDirichlet from traceability.vsm import measurement","title":"Module traceability"},{"location":"reference/traceability/#sub-modules","text":"traceability.lda traceability.lsa traceability.preprocessing_evaluation traceability.vsm","title":"Sub-modules"},{"location":"reference/traceability/lda/","text":"Module traceability.lda None None View Source import pandas as pd from scipy.sparse import data from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer from sklearn.decomposition import NMF , LatentDirichletAllocation from traceability.preprocessing_evaluation import pengukuranEvaluasi from tabulate import tabulate class latentDirichlet : def __init__ ( self , data_raw ): self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ): # Use tf-idf features for NMF. tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ): tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ): lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self )[ 1 ]) tf_feature_names = latentDirichlet . ukur_tf ( self )[ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : myLDA = latentDirichlet ( data ) dt_fr = myLDA . Frobenius_norm_feature ( data ) print ( tabulate ( dt_fr , headers = 'keys' , tablefmt = 'psql' )) th_fr = myLDA . threshold_value ( 0.2 , dt_fr ) print ( tabulate ( th_fr , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) hasil_ukur1 = myUkur . ukur_evaluasi () dt_kl = myLDA . Kullback_feature ( data ) print ( tabulate ( dt_kl , headers = 'keys' , tablefmt = 'psql' )) th_kl = myLDA . threshold_value ( 0.2 , dt_kl ) print ( tabulate ( th_kl , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) hasil_ukur2 = myUkur . ukur_evaluasi () dt_lda = myLDA . lda_feature ( data ) print ( tabulate ( dt_lda , headers = 'keys' , tablefmt = 'psql' )) th_lda = myLDA . threshold_value ( 0.2 , dt_lda ) print ( tabulate ( th_lda , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) hasil_ukur3 = myUkur . ukur_evaluasi () Classes latentDirichlet class latentDirichlet ( data_raw ) View Source class latentDirichlet : def __init__ ( self , data_raw ) : self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ) : # Use tf - idf features for NMF . tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 Methods Frobenius_norm_feature def Frobenius_norm_feature ( self , req ) View Source def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df Kullback_feature def Kullback_feature ( self , req ) View Source def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df lda_feature def lda_feature ( self , req ) View Source def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ) , max_iter = 5 , learning_method = ' online ' , learning_offset = 50 ., random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ]. get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 ukur_tf def ukur_tf ( self ) View Source def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf ukur_tfidf_vectorizer def ukur_tfidf_vectorizer ( self ) View Source def ukur_tfidf_vectorizer ( self ) : # Use tf - idf features for NMF . tfidf_vectorizer = TfidfVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf","title":"Lda"},{"location":"reference/traceability/lda/#module-traceabilitylda","text":"None None View Source import pandas as pd from scipy.sparse import data from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer from sklearn.decomposition import NMF , LatentDirichletAllocation from traceability.preprocessing_evaluation import pengukuranEvaluasi from tabulate import tabulate class latentDirichlet : def __init__ ( self , data_raw ): self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ): # Use tf-idf features for NMF. tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ): tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ): nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self )[ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ): lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self )[ 1 ]) tf_feature_names = latentDirichlet . ukur_tf ( self )[ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self )[ 1 ]) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : myLDA = latentDirichlet ( data ) dt_fr = myLDA . Frobenius_norm_feature ( data ) print ( tabulate ( dt_fr , headers = 'keys' , tablefmt = 'psql' )) th_fr = myLDA . threshold_value ( 0.2 , dt_fr ) print ( tabulate ( th_fr , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_fr , th_fr ) hasil_ukur1 = myUkur . ukur_evaluasi () dt_kl = myLDA . Kullback_feature ( data ) print ( tabulate ( dt_kl , headers = 'keys' , tablefmt = 'psql' )) th_kl = myLDA . threshold_value ( 0.2 , dt_kl ) print ( tabulate ( th_kl , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_kl , th_kl ) hasil_ukur2 = myUkur . ukur_evaluasi () dt_lda = myLDA . lda_feature ( data ) print ( tabulate ( dt_lda , headers = 'keys' , tablefmt = 'psql' )) th_lda = myLDA . threshold_value ( 0.2 , dt_lda ) print ( tabulate ( th_lda , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_lda , th_lda ) hasil_ukur3 = myUkur . ukur_evaluasi ()","title":"Module traceability.lda"},{"location":"reference/traceability/lda/#classes","text":"","title":"Classes"},{"location":"reference/traceability/lda/#latentdirichlet","text":"class latentDirichlet ( data_raw ) View Source class latentDirichlet : def __init__ ( self , data_raw ) : self . data = data_raw self . n_features = len ( self . data ) def ukur_tfidf_vectorizer ( self ) : # Use tf - idf features for NMF . tfidf_vectorizer = TfidfVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0.95 , min_df = 2 , max_features = self . n_features , stop_words = 'english' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ] . get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ), max_iter = 5 , learning_method = 'online' , learning_offset = 50. , random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ] . get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ), random_state = 1 , beta_loss = 'kullback-leibler' , solver = 'mu' , max_iter = 1000 , alpha = .1 , l1_ratio = .5 ). fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"latentDirichlet"},{"location":"reference/traceability/lda/#methods","text":"","title":"Methods"},{"location":"reference/traceability/lda/#frobenius_norm_feature","text":"def Frobenius_norm_feature ( self , req ) View Source def Frobenius_norm_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) nmf_tfidf = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_frb_tfidf = ( nmf_tfidf ) data_frb_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_frb_tfidf , index = req , columns = fitur_frb_tfidf ) return dt_df","title":"Frobenius_norm_feature"},{"location":"reference/traceability/lda/#kullback_feature","text":"def Kullback_feature ( self , req ) View Source def Kullback_feature ( self , req ) : nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) tfidf_feature_names = latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 0 ]. get_feature_names () fitur_kll_tfidfi = ( tfidf_feature_names ) data_kll_tfidf = ( nmf . components_ ) dt_df = pd . DataFrame ( data_kll_tfidf , index = req , columns = fitur_kll_tfidfi ) return dt_df","title":"Kullback_feature"},{"location":"reference/traceability/lda/#lda_feature","text":"def lda_feature ( self , req ) View Source def lda_feature ( self , req ) : lda = LatentDirichletAllocation ( n_components = len ( self . data ) , max_iter = 5 , learning_method = ' online ' , learning_offset = 50 ., random_state = 0 ) lda . fit ( latentDirichlet . ukur_tf ( self ) [ 1 ] ) tf_feature_names = latentDirichlet . ukur_tf ( self ) [ 0 ]. get_feature_names () fitur_lda = ( tf_feature_names ) nmf = NMF ( n_components = len ( self . data ) , random_state = 1 , beta_loss = ' kullback-leibler ' , solver = ' mu ' , max_iter = 1000 , alpha = . 1 , l1_ratio = . 5 ) . fit ( latentDirichlet . ukur_tfidf_vectorizer ( self ) [ 1 ] ) data_lda = ( nmf . components_ ) dt_df = pd . DataFrame ( data_lda , index = req , columns = fitur_lda ) return dt_df","title":"lda_feature"},{"location":"reference/traceability/lda/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"},{"location":"reference/traceability/lda/#ukur_tf","text":"def ukur_tf ( self ) View Source def ukur_tf ( self ) : tf_vectorizer = CountVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tf = tf_vectorizer . fit_transform ( self . data ) return tf_vectorizer , tf","title":"ukur_tf"},{"location":"reference/traceability/lda/#ukur_tfidf_vectorizer","text":"def ukur_tfidf_vectorizer ( self ) View Source def ukur_tfidf_vectorizer ( self ) : # Use tf - idf features for NMF . tfidf_vectorizer = TfidfVectorizer ( max_df = 0 . 95 , min_df = 2 , max_features = self . n_features , stop_words = ' english ' ) tfidf = tfidf_vectorizer . fit_transform ( self . data ) return tfidf_vectorizer , tfidf","title":"ukur_tfidf_vectorizer"},{"location":"reference/traceability/lsa/","text":"Module traceability.lsa None None View Source import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from traceability.preprocessing_evaluation import pengukuranEvaluasi from tabulate import tabulate class latentSemantic : def __init__ ( self , data_raw ): self . data = data_raw def ukurLSA ( self , req ): vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : # lsa measurement myLSA = latentSemantic () dt_lsa = myLSA . ukurLSA () print ( tabulate ( dt_lsa , headers = 'keys' , tablefmt = 'psql' )) th_lsa = myLSA . threshold_value ( 0.2 , dt_lsa ) print ( tabulate ( th_lsa , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) hasil_ukur3 = myUkur . ukur_evaluasi () Classes latentSemantic class latentSemantic ( data_raw ) View Source class latentSemantic : def __init__ ( self , data_raw ) : self . data = data_raw def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 Methods threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 ukurLSA def ukurLSA ( self , req ) View Source def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = ' english ' , max_features = 1000 , # keep top 1000 terms max_df = 0 . 5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ) , algorithm = ' randomized ' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms )","title":"Lsa"},{"location":"reference/traceability/lsa/#module-traceabilitylsa","text":"None None View Source import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from traceability.preprocessing_evaluation import pengukuranEvaluasi from tabulate import tabulate class latentSemantic : def __init__ ( self , data_raw ): self . data = data_raw def ukurLSA ( self , req ): vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : # lsa measurement myLSA = latentSemantic () dt_lsa = myLSA . ukurLSA () print ( tabulate ( dt_lsa , headers = 'keys' , tablefmt = 'psql' )) th_lsa = myLSA . threshold_value ( 0.2 , dt_lsa ) print ( tabulate ( th_lsa , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_lsa , th_lsa ) hasil_ukur3 = myUkur . ukur_evaluasi ()","title":"Module traceability.lsa"},{"location":"reference/traceability/lsa/#classes","text":"","title":"Classes"},{"location":"reference/traceability/lsa/#latentsemantic","text":"class latentSemantic ( data_raw ) View Source class latentSemantic : def __init__ ( self , data_raw ) : self . data = data_raw def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = 'english' , max_features = 1000 , # keep top 1000 terms max_df = 0.5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ), algorithm = 'randomized' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms ) def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"latentSemantic"},{"location":"reference/traceability/lsa/#methods","text":"","title":"Methods"},{"location":"reference/traceability/lsa/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"},{"location":"reference/traceability/lsa/#ukurlsa","text":"def ukurLSA ( self , req ) View Source def ukurLSA ( self , req ) : vectorizer = TfidfVectorizer ( stop_words = ' english ' , max_features = 1000 , # keep top 1000 terms max_df = 0 . 5 , smooth_idf = True ) X = vectorizer . fit_transform ( self . data ) svd_model = TruncatedSVD ( n_components = len ( self . data ) , algorithm = ' randomized ' , n_iter = 100 , random_state = 122 ) svd_model . fit ( X ) terms = vectorizer . get_feature_names () return pd . DataFrame ( svd_model . components_ , index = req , columns = terms )","title":"ukurLSA"},{"location":"reference/traceability/preprocessing_evaluation/","text":"Module traceability.preprocessing_evaluation None None View Source import pandas as pd import pandas as pd import numpy as np from spacy.lang.en import English from tabulate import tabulate from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler from pyAutoML.ml import ML , ml , EncodeCategorical from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split class prosesData : def __init__ ( self , namaFile ): self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ): xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ): xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ): cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X def clean_text ( self , raw_text ): nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ): self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ): XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2: Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )): predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all (): correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {} \" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ): X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70% training and 30% test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0.4 return ML ( X , Y , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) if __name__ == \"__main__\" : myData = prosesData () # myData.preprocessing() req = myData . fulldataset () # myData.fulldataset(inputSRS) text_to_clean = list ( req [ 'Requirement Statement' ]) cleaned_text = myData . apply_cleaning_function_to_list ( text_to_clean ) Classes pengukuranEvaluasi class pengukuranEvaluasi ( dataPertama , dataKedua ) View Source class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ) : self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ) : XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0.4 return ML ( X , Y , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) Methods kmeans_cluster def kmeans_cluster ( self , nilai_cluster = 3 ) View Source def kmeans_cluster ( self , nilai_cluster = 3 ) : XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) ukur_evaluasi def ukur_evaluasi ( self ) View Source def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ) , self . data2 , test_size = 0 . 3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0 . 4 return ML ( X , Y , size , SVC () , RandomForestClassifier () , DecisionTreeClassifier () , KNeighborsClassifier () , LogisticRegression ( max_iter = 7000 )) prosesData class prosesData ( namaFile ) View Source class prosesData : def __init__ ( self , namaFile ) : self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ) : xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ) : xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ) : cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X def clean_text ( self , raw_text ) : nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token.lemma_.lower() for token in tokens if token.is_stop is False and token.is_punct is False and token.is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words Methods apply_cleaning_function_to_list def apply_cleaning_function_to_list ( self , X ) View Source def apply_cleaning_function_to_list ( self , X ) : cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X clean_text def clean_text ( self , raw_text ) View Source def clean_text ( self , raw_text ) : nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words fulldataset def fulldataset ( self , inputSRS = 'SRS1' ) View Source def fulldataset ( self , inputSRS = 'SRS1' ) : xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua preprocessing def preprocessing ( self ) View Source def preprocessing ( self ) : xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"Preprocessing Evaluation"},{"location":"reference/traceability/preprocessing_evaluation/#module-traceabilitypreprocessing_evaluation","text":"None None View Source import pandas as pd import pandas as pd import numpy as np from spacy.lang.en import English from tabulate import tabulate from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler from pyAutoML.ml import ML , ml , EncodeCategorical from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split class prosesData : def __init__ ( self , namaFile ): self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ): xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ): xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [ {} ] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ): cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X def clean_text ( self , raw_text ): nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ): self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ): XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2: Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )): predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all (): correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {} \" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ): X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70% training and 30% test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0.4 return ML ( X , Y , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 )) if __name__ == \"__main__\" : myData = prosesData () # myData.preprocessing() req = myData . fulldataset () # myData.fulldataset(inputSRS) text_to_clean = list ( req [ 'Requirement Statement' ]) cleaned_text = myData . apply_cleaning_function_to_list ( text_to_clean )","title":"Module traceability.preprocessing_evaluation"},{"location":"reference/traceability/preprocessing_evaluation/#classes","text":"","title":"Classes"},{"location":"reference/traceability/preprocessing_evaluation/#pengukuranevaluasi","text":"class pengukuranEvaluasi ( dataPertama , dataKedua ) View Source class pengukuranEvaluasi : def __init__ ( self , dataPertama , dataKedua ) : self . data1 = dataPertama self . data2 = dataKedua def kmeans_cluster ( self , nilai_cluster = 3 ) : XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled ) def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ), self . data2 , test_size = 0.3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0.4 return ML ( X , Y , size , SVC (), RandomForestClassifier (), DecisionTreeClassifier (), KNeighborsClassifier (), LogisticRegression ( max_iter = 7000 ))","title":"pengukuranEvaluasi"},{"location":"reference/traceability/preprocessing_evaluation/#methods","text":"","title":"Methods"},{"location":"reference/traceability/preprocessing_evaluation/#kmeans_cluster","text":"def kmeans_cluster ( self , nilai_cluster = 3 ) View Source def kmeans_cluster ( self , nilai_cluster = 3 ) : XVSM = np . array ( self . data1 ) yVSM = np . array ( self . data2 ) kmeans = KMeans ( n_clusters = nilai_cluster ) # You want cluster the passenger records into 2 : Survived or Not survived kmeans . fit ( XVSM ) correct = 0 for i in range ( len ( XVSM )) : predict_me = np . array ( XVSM [ i ] . astype ( float )) predict_me = predict_me . reshape ( - 1 , len ( predict_me )) prediction = kmeans . predict ( predict_me ) if prediction [ 0 ] == yVSM . all () : correct += 1 scaler = MinMaxScaler () XVSM_scaled = scaler . fit_transform ( yVSM ) print ( \"data_correction {}\" . format ( correct / len ( XVSM ))) return ( XVSM_scaled )","title":"kmeans_cluster"},{"location":"reference/traceability/preprocessing_evaluation/#ukur_evaluasi","text":"def ukur_evaluasi ( self ) View Source def ukur_evaluasi ( self ) : X_train , X_test , y_train , y_test = train_test_split ( pengukuranEvaluasi . kmeans_cluster ( self ) , self . data2 , test_size = 0 . 3 , random_state = 109 ) # 70 % training and 30 % test y_train = y_train . argmax ( axis = 1 ) X = X_train Y = y_train Y = EncodeCategorical ( Y ) size = 0 . 4 return ML ( X , Y , size , SVC () , RandomForestClassifier () , DecisionTreeClassifier () , KNeighborsClassifier () , LogisticRegression ( max_iter = 7000 ))","title":"ukur_evaluasi"},{"location":"reference/traceability/preprocessing_evaluation/#prosesdata","text":"class prosesData ( namaFile ) View Source class prosesData : def __init__ ( self , namaFile ) : self . dataFile = namaFile def fulldataset ( self , inputSRS = 'SRS1' ) : xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua def preprocessing ( self ) : xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( 'Processing: [{}] ...' . format ( sh )) print ( df . head ()) def apply_cleaning_function_to_list ( self , X ) : cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X def clean_text ( self , raw_text ) : nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token.lemma_.lower() for token in tokens if token.is_stop is False and token.is_punct is False and token.is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words","title":"prosesData"},{"location":"reference/traceability/preprocessing_evaluation/#methods_1","text":"","title":"Methods"},{"location":"reference/traceability/preprocessing_evaluation/#apply_cleaning_function_to_list","text":"def apply_cleaning_function_to_list ( self , X ) View Source def apply_cleaning_function_to_list ( self , X ) : cleaned_X = [] for element in X : cleaned_X . append ( prosesData . clean_text ( self , raw_text = element )) return cleaned_X","title":"apply_cleaning_function_to_list"},{"location":"reference/traceability/preprocessing_evaluation/#clean_text","text":"def clean_text ( self , raw_text ) View Source def clean_text ( self , raw_text ) : nlp = English () tokenizer = nlp . Defaults . create_tokenizer ( nlp ) tokens = tokenizer ( raw_text ) lemma_list = [ token . lemma_ . lower () for token in tokens if token . is_stop is False and token . is_punct is False and token . is_alpha is True ] joined_words = ( \" \" . join ( lemma_list )) return joined_words","title":"clean_text"},{"location":"reference/traceability/preprocessing_evaluation/#fulldataset","text":"def fulldataset ( self , inputSRS = 'SRS1' ) View Source def fulldataset ( self , inputSRS = 'SRS1' ) : xl = pd . ExcelFile ( self . dataFile ) dfs = { sh : xl . parse ( sh ) for sh in xl . sheet_names } kalimat = dfs [ inputSRS ] kalimat_semua = kalimat . head ( len ( kalimat )) return kalimat_semua","title":"fulldataset"},{"location":"reference/traceability/preprocessing_evaluation/#preprocessing","text":"def preprocessing ( self ) View Source def preprocessing ( self ) : xl = pd . ExcelFile ( self . dataFile ) for sh in xl . sheet_names : df = xl . parse ( sh ) print ( ' Processing: [{}] ... ' . format ( sh )) print ( df . head ())","title":"preprocessing"},{"location":"reference/traceability/vsm/","text":"Module traceability.vsm None None View Source import numpy as np import pandas as pd import math from tabulate import tabulate from scipy.sparse import data from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler from traceability.preprocessing_evaluation import pengukuranEvaluasi class measurement : def __init__ ( self , cleaned_data ): self . data = cleaned_data def bagOfWords ( self , data_raw , req ): b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya, sebagai kolom e = b . transform ( data_raw ) . toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data, indeks, kolom return bow_df def l2_normalizer ( self , vec ): denom = np . sum ([ el ** 2 for el in vec ]) return [( el / math . sqrt ( denom )) for el in vec ] def build_lexicon ( self , corpus ): lexicon = set () for doc in corpus : lexicon . update ([ word for word in doc . split ()]) return lexicon def freq ( self , term , document ): return document . split () . count ( term ) def numDocsContaining ( self , word , doclist ): doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ): n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ): idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ): X = np . array ( data [ 0 :]) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : myVSMMeasurement = measurement ( data ) mydoclist = data bow = myVSMMeasurement . bagOfWords ( mydoclist ) vocabulary = myVSMMeasurement . build_lexicon ( mydoclist ) # tfidf normal my_idf_vector = [ myVSMMeasurement . idf ( word , mydoclist ) for word in vocabulary ] # vektor idf my_idf_matrix = myVSMMeasurement . build_idf_matrix ( my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = myVSMMeasurement . cosine_measurement ( doc_term_matrix_tfidf . values ) th_cosine = myVSMMeasurement . threshold_value ( 0.2 , dt_cosine ) print ( tabulate ( dt_cosine , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) hasil_ukur1 = myUkur . ukur_evaluasi () # tfidf dengan l2 normalizer doc_term_matrix_l2 = [ myVSMMeasurement . l2_normalizer ( vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ myVSMMeasurement . l2_normalizer ( tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = myVSMMeasurement . cosine_measurement ( doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = myVSMMeasurement . threshold_value ( 0.2 , dt_cosine_l2 ) print ( tabulate ( dt_cosine , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) hasil_ukur2 = myUkur . ukur_evaluasi () Classes measurement class measurement ( cleaned_data ) View Source class measurement : def __init__ ( self , cleaned_data ) : self . data = cleaned_data def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ). toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data , indeks , kolom return bow_df def l2_normalizer ( self , vec ) : denom = np . sum ( [ el**2 for el in vec ] ) return [ (el / math.sqrt(denom)) for el in vec ] def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc.split() ] ) return lexicon def freq ( self , term , document ) : return document . split (). count ( term ) def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0: ] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 Methods bagOfWords def bagOfWords ( self , data_raw , req ) View Source def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ) . toarray () # data bow_df = pd . DataFrame ( e , req , d ) # data , indeks , kolom return bow_df build_idf_matrix def build_idf_matrix ( self , idf_vector ) View Source def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ) , len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat build_lexicon def build_lexicon ( self , corpus ) View Source def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc . split () ] ) return lexicon cosine_measurement def cosine_measurement ( self , data , req ) View Source def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0 :] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = ' linear ' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine freq def freq ( self , term , document ) View Source def freq ( self , term , document ) : return document . split () . count ( term ) idf def idf ( self , word , doclist ) View Source def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) l2_normalizer def l2_normalizer ( self , vec ) View Source def l2_normalizer ( self , vec ) : denom = np . sum ( [ el ** 2 for el in vec ] ) return [ ( el / math . sqrt ( denom )) for el in vec ] numDocsContaining def numDocsContaining ( self , word , doclist ) View Source def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount threshold_value def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"Vsm"},{"location":"reference/traceability/vsm/#module-traceabilityvsm","text":"None None View Source import numpy as np import pandas as pd import math from tabulate import tabulate from scipy.sparse import data from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler from traceability.preprocessing_evaluation import pengukuranEvaluasi class measurement : def __init__ ( self , cleaned_data ): self . data = cleaned_data def bagOfWords ( self , data_raw , req ): b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya, sebagai kolom e = b . transform ( data_raw ) . toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data, indeks, kolom return bow_df def l2_normalizer ( self , vec ): denom = np . sum ([ el ** 2 for el in vec ]) return [( el / math . sqrt ( denom )) for el in vec ] def build_lexicon ( self , corpus ): lexicon = set () for doc in corpus : lexicon . update ([ word for word in doc . split ()]) return lexicon def freq ( self , term , document ): return document . split () . count ( term ) def numDocsContaining ( self , word , doclist ): doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ): n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ): idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ): X = np . array ( data [ 0 :]) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ): dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ([ True ]) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ([ False ]) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1 if __name__ == \"__main__\" : myVSMMeasurement = measurement ( data ) mydoclist = data bow = myVSMMeasurement . bagOfWords ( mydoclist ) vocabulary = myVSMMeasurement . build_lexicon ( mydoclist ) # tfidf normal my_idf_vector = [ myVSMMeasurement . idf ( word , mydoclist ) for word in vocabulary ] # vektor idf my_idf_matrix = myVSMMeasurement . build_idf_matrix ( my_idf_vector ) # membuat matriks idf doc_term_matrix_tfidf = [ np . dot ( tf_vector , my_idf_matrix ) for tf_vector in bow . values ] dt_cosine = myVSMMeasurement . cosine_measurement ( doc_term_matrix_tfidf . values ) th_cosine = myVSMMeasurement . threshold_value ( 0.2 , dt_cosine ) print ( tabulate ( dt_cosine , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_cosine , th_cosine ) hasil_ukur1 = myUkur . ukur_evaluasi () # tfidf dengan l2 normalizer doc_term_matrix_l2 = [ myVSMMeasurement . l2_normalizer ( vec ) for vec in bow . values ] doc_term_matrix_tfidf_l2 = [ myVSMMeasurement . l2_normalizer ( tf_vector ) for tf_vector in doc_term_matrix_tfidf ] dt_cosine_l2 = myVSMMeasurement . cosine_measurement ( doc_term_matrix_tfidf_l2 . values ) th_cosine_l2 = myVSMMeasurement . threshold_value ( 0.2 , dt_cosine_l2 ) print ( tabulate ( dt_cosine , headers = 'keys' , tablefmt = 'psql' )) myUkur = pengukuranEvaluasi ( dt_cosine_l2 , th_cosine_l2 ) hasil_ukur2 = myUkur . ukur_evaluasi ()","title":"Module traceability.vsm"},{"location":"reference/traceability/vsm/#classes","text":"","title":"Classes"},{"location":"reference/traceability/vsm/#measurement","text":"class measurement ( cleaned_data ) View Source class measurement : def __init__ ( self , cleaned_data ) : self . data = cleaned_data def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ). toarray () #data bow_df = pd . DataFrame ( e , req , d ) #data , indeks , kolom return bow_df def l2_normalizer ( self , vec ) : denom = np . sum ( [ el**2 for el in vec ] ) return [ (el / math.sqrt(denom)) for el in vec ] def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc.split() ] ) return lexicon def freq ( self , term , document ) : return document . split (). count ( term ) def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df ) def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ), len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0: ] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = 'linear' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"measurement"},{"location":"reference/traceability/vsm/#methods","text":"","title":"Methods"},{"location":"reference/traceability/vsm/#bagofwords","text":"def bagOfWords ( self , data_raw , req ) View Source def bagOfWords ( self , data_raw , req ) : b = CountVectorizer ( data_raw ) # dilakukan vektorisasi c = b . fit ( data_raw ) # dilakukan fiting d = b . get_feature_names () # diambil namanya , sebagai kolom e = b . transform ( data_raw ) . toarray () # data bow_df = pd . DataFrame ( e , req , d ) # data , indeks , kolom return bow_df","title":"bagOfWords"},{"location":"reference/traceability/vsm/#build_idf_matrix","text":"def build_idf_matrix ( self , idf_vector ) View Source def build_idf_matrix ( self , idf_vector ) : idf_mat = np . zeros (( len ( idf_vector ) , len ( idf_vector ))) np . fill_diagonal ( idf_mat , idf_vector ) return idf_mat","title":"build_idf_matrix"},{"location":"reference/traceability/vsm/#build_lexicon","text":"def build_lexicon ( self , corpus ) View Source def build_lexicon ( self , corpus ) : lexicon = set () for doc in corpus : lexicon . update ( [ word for word in doc . split () ] ) return lexicon","title":"build_lexicon"},{"location":"reference/traceability/vsm/#cosine_measurement","text":"def cosine_measurement ( self , data , req ) View Source def cosine_measurement ( self , data , req ) : X = np . array ( data [ 0 :] ) Y = np . array ( data ) cosine_similaritas = pairwise_kernels ( X , Y , metric = ' linear ' ) frequency_cosine = pd . DataFrame ( cosine_similaritas , index = req , columns = req ) return frequency_cosine","title":"cosine_measurement"},{"location":"reference/traceability/vsm/#freq","text":"def freq ( self , term , document ) View Source def freq ( self , term , document ) : return document . split () . count ( term )","title":"freq"},{"location":"reference/traceability/vsm/#idf","text":"def idf ( self , word , doclist ) View Source def idf ( self , word , doclist ) : n_samples = len ( doclist ) df = measurement . numDocsContaining ( self , word , doclist ) return np . log ( n_samples / 1 + df )","title":"idf"},{"location":"reference/traceability/vsm/#l2_normalizer","text":"def l2_normalizer ( self , vec ) View Source def l2_normalizer ( self , vec ) : denom = np . sum ( [ el ** 2 for el in vec ] ) return [ ( el / math . sqrt ( denom )) for el in vec ]","title":"l2_normalizer"},{"location":"reference/traceability/vsm/#numdocscontaining","text":"def numDocsContaining ( self , word , doclist ) View Source def numDocsContaining ( self , word , doclist ) : doccount = 0 for doc in doclist : if measurement . freq ( self , term = word , document = doc ) > 0 : doccount += 1 return doccount","title":"numDocsContaining"},{"location":"reference/traceability/vsm/#threshold_value","text":"def threshold_value ( self , threshold , data ) View Source def threshold_value ( self , threshold , data ) : dt = data . values >= threshold dt1 = pd . DataFrame ( dt , index = data . index , columns = data . columns ) mask = dt1 . isin ( [ True ] ) dt3 = dt1 . where ( mask , other = 0 ) mask2 = dt3 . isin ( [ False ] ) th_cosine1 = dt3 . where ( mask2 , other = 1 ) return th_cosine1","title":"threshold_value"}]}